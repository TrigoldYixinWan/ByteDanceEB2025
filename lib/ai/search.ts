/**
 * è¯­ä¹‰æœç´¢æœåŠ¡
 * 
 * ä½¿ç”¨ pgvector è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
 * æ”¯æŒ Multi-Query æ‰©å±•æœç´¢
 */

import { createClient } from '@/lib/supabase/server'
import { generateEmbedding, generateEmbeddingBatch } from './embedding'
import OpenAI from 'openai'

// OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºæŸ¥è¯¢æ‰©å±•ï¼‰
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})

/**
 * æœç´¢ç»“æœæ¥å£
 */
export interface SearchResult {
  chunkId: string
  documentId: string
  documentTitle: string
  documentCategory: string
  content: string
  similarity: number
}

/**
 * è¯­ä¹‰æœç´¢ - æ ¹æ®æŸ¥è¯¢æ–‡æœ¬æœç´¢ç›¸å…³æ–‡æ¡£å—
 * 
 * @param query ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
 * @param limit è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤ 8
 * @param threshold ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ 0.7
 * @returns æœç´¢ç»“æœæ•°ç»„
 */
export async function semanticSearch(
  query: string,
  limit: number = 8,
  threshold: number = 0.7
): Promise<SearchResult[]> {
  console.log(`ğŸ” å¼€å§‹è¯­ä¹‰æœç´¢: "${query.slice(0, 50)}..."`)
  
  try {
    // Step 1: ç”ŸæˆæŸ¥è¯¢å‘é‡
    const queryEmbedding = await generateEmbedding(query)
    console.log(`âœ… æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆ (${queryEmbedding.length} ç»´)`)
    
    // Step 2: ä½¿ç”¨ pgvector è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
    const supabase = await createClient()
    
    // ä½¿ç”¨ RPC å‡½æ•°è¿›è¡Œå‘é‡æœç´¢ï¼ˆéœ€è¦åœ¨æ•°æ®åº“ä¸­åˆ›å»ºï¼‰
    // æˆ–è€…ä½¿ç”¨åŸç”Ÿ SQL æŸ¥è¯¢
    const { data: results, error } = await supabase.rpc('match_documents', {
      query_embedding: queryEmbedding,
      match_threshold: threshold,
      match_count: limit,
    })
    
    if (error) {
      // å¦‚æœ RPC å‡½æ•°ä¸å­˜åœ¨ï¼Œå›é€€åˆ°ç›´æ¥æŸ¥è¯¢
      console.warn('âš ï¸ RPC å‡½æ•°ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æŸ¥è¯¢æ–¹æ¡ˆ')
      return await fallbackSearch(supabase, queryEmbedding, limit, threshold)
    }
    
    console.log(`âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° ${results?.length || 0} ä¸ªç›¸å…³ç»“æœ`)
    
    return (results || []).map((r: any) => ({
      chunkId: r.chunk_id,
      documentId: r.document_id,
      documentTitle: r.document_title,
      documentCategory: r.document_category,
      content: r.content,
      similarity: r.similarity,
    }))
  } catch (error) {
    console.error('âŒ è¯­ä¹‰æœç´¢å¤±è´¥:', error)
    throw error
  }
}

/**
 * å¤‡ç”¨æœç´¢æ–¹æ¡ˆ - ä¸ä½¿ç”¨ RPC å‡½æ•°
 */
async function fallbackSearch(
  supabase: any,
  queryEmbedding: number[],
  limit: number,
  threshold: number
): Promise<SearchResult[]> {
  // å°†å‘é‡è½¬æ¢ä¸º PostgreSQL æ ¼å¼
  const embeddingStr = `[${queryEmbedding.join(',')}]`
  
  // ä½¿ç”¨åŸç”ŸæŸ¥è¯¢
  // æ³¨æ„ï¼šè¿™éœ€è¦ document_chunks è¡¨æœ‰æ­£ç¡®çš„ç´¢å¼•
  const { data: chunks, error: chunksError } = await supabase
    .from('document_chunks')
    .select(`
      id,
      document_id,
      content,
      documents!inner (
        id,
        title,
        category,
        status
      )
    `)
    .eq('documents.status', 'ready')
    .limit(limit * 3)  // è·å–æ›´å¤šï¼Œç„¶ååœ¨åº”ç”¨å±‚è¿‡æ»¤
  
  if (chunksError) {
    console.error('âŒ å¤‡ç”¨æœç´¢æŸ¥è¯¢å¤±è´¥:', chunksError)
    throw new Error(`æœç´¢å¤±è´¥: ${chunksError.message}`)
  }
  
  // ç”±äº Supabase JS å®¢æˆ·ç«¯ä¸ç›´æ¥æ”¯æŒå‘é‡æ“ä½œ
  // æˆ‘ä»¬éœ€è¦ä½¿ç”¨ SQL æŸ¥è¯¢
  // è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ–¹æ¡ˆï¼šè¿”å›æ‰€æœ‰å—ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†
  console.warn('âš ï¸ å¤‡ç”¨æœç´¢æ— æ³•è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå»ºè®®åˆ›å»º RPC å‡½æ•°')
  
  // è¿”å›ç»“æœï¼ˆæ²¡æœ‰ç›¸ä¼¼åº¦æ’åºï¼‰
  return (chunks || []).slice(0, limit).map((chunk: any) => ({
    chunkId: chunk.id,
    documentId: chunk.document_id,
    documentTitle: chunk.documents?.title || 'æœªçŸ¥æ–‡æ¡£',
    documentCategory: chunk.documents?.category || 'æœªçŸ¥ç±»åˆ«',
    content: chunk.content,
    similarity: 0.8,  // å ä½å€¼
  }))
}

/**
 * ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡
 */
export function estimateTokens(text: string): number {
  // ç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡çº¦ 1.5 å­—ç¬¦ = 1 tokenï¼Œè‹±æ–‡çº¦ 4 å­—ç¬¦ = 1 token
  // ä½¿ç”¨ä¿å®ˆä¼°è®¡
  return Math.ceil(text.length / 2)
}

/**
 * å»é‡å¹¶åˆå¹¶æœç´¢ç»“æœ
 * 
 * @param results æœç´¢ç»“æœ
 * @param maxTokens æœ€å¤§ token æ•°
 * @returns å»é‡åçš„ç»“æœ
 */
export function deduplicateResults(
  results: SearchResult[],
  maxTokens: number = 4000
): SearchResult[] {
  const seen = new Set<string>()
  const deduplicated: SearchResult[] = []
  let totalTokens = 0
  
  for (const result of results) {
    // è·³è¿‡é‡å¤çš„å—
    if (seen.has(result.chunkId)) continue
    
    // æ£€æŸ¥ token é¢„ç®—
    const chunkTokens = estimateTokens(result.content)
    if (totalTokens + chunkTokens > maxTokens) {
      console.log(`âš ï¸ Token é¢„ç®—å·²æ»¡ï¼Œåœæ­¢æ·»åŠ æ›´å¤šç»“æœ`)
      break
    }
    
    seen.add(result.chunkId)
    deduplicated.push(result)
    totalTokens += chunkTokens
  }
  
  console.log(`ğŸ“Š å»é‡å: ${deduplicated.length} ä¸ªç»“æœ, ~${totalTokens} tokens`)
  return deduplicated
}

// ============================================================
// Multi-Query æ‰©å±•æœç´¢
// ============================================================

/**
 * ç”ŸæˆæŸ¥è¯¢å˜ä½“ï¼ˆMulti-Queryï¼‰
 * 
 * ä½¿ç”¨ LLM å°†ç”¨æˆ·é—®é¢˜æ”¹å†™ä¸ºå¤šä¸ªä¸åŒè§’åº¦çš„æŸ¥è¯¢
 * 
 * @param originalQuery åŸå§‹ç”¨æˆ·é—®é¢˜
 * @param maxQueries æœ€å¤§æŸ¥è¯¢æ•°é‡ï¼ˆåŒ…å«åŸå§‹æŸ¥è¯¢ï¼‰ï¼Œé»˜è®¤ 4ï¼Œæœ€å¤§ 5
 * @returns æŸ¥è¯¢æ•°ç»„ï¼ˆåŒ…å«åŸå§‹æŸ¥è¯¢ï¼‰
 */
export async function generateQueryVariants(
  originalQuery: string,
  maxQueries: number = 4
): Promise<string[]> {
  // é™åˆ¶æœ€å¤§æŸ¥è¯¢æ•°é‡
  const limit = Math.min(maxQueries, 5)
  const variantsNeeded = limit - 1  // å‡å»åŸå§‹æŸ¥è¯¢
  
  if (variantsNeeded <= 0) {
    return [originalQuery]
  }

  console.log(`ğŸ”„ ç”Ÿæˆ ${variantsNeeded} ä¸ªæŸ¥è¯¢å˜ä½“...`)

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: `ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ‰©å±•åŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šç»™ä½ ä¸€ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦ç”Ÿæˆ ${variantsNeeded} ä¸ªç›¸å…³ä½†ä¸åŒè§’åº¦çš„æŸ¥è¯¢ï¼Œå¸®åŠ©åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªæŸ¥è¯¢åº”è¯¥ä»ä¸åŒè§’åº¦è¡¨è¾¾ç›¸åŒæˆ–ç›¸å…³çš„ä¿¡æ¯éœ€æ±‚
2. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯å’Œè¡¨è¾¾æ–¹å¼
3. è€ƒè™‘åŒä¹‰è¯ã€ç›¸å…³æ¦‚å¿µã€ä¸Šä¸‹ä½æ¦‚å¿µ
4. ä¿æŒæŸ¥è¯¢ç®€æ´ï¼Œ10-30å­—ä¸ºå®œ
5. å¿…é¡»æ˜¯ä¸­æ–‡

ç›´æ¥è¿”å› JSON æ•°ç»„æ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š
["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]`
        },
        {
          role: 'user',
          content: originalQuery
        }
      ],
      temperature: 0.7,
      max_tokens: 300,
    })

    const content = response.choices[0]?.message?.content?.trim() || '[]'
    
    // è§£æ JSON
    let variants: string[] = []
    try {
      // å°è¯•æå– JSON æ•°ç»„
      const jsonMatch = content.match(/\[[\s\S]*\]/)
      if (jsonMatch) {
        variants = JSON.parse(jsonMatch[0])
      }
    } catch (e) {
      console.warn('âš ï¸ è§£ææŸ¥è¯¢å˜ä½“å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢:', e)
    }

    // è¿‡æ»¤æ— æ•ˆæŸ¥è¯¢
    variants = variants
      .filter(q => typeof q === 'string' && q.trim().length > 0)
      .slice(0, variantsNeeded)

    // åˆå¹¶åŸå§‹æŸ¥è¯¢å’Œå˜ä½“
    const allQueries = [originalQuery, ...variants]
    
    console.log(`âœ… ç”ŸæˆæŸ¥è¯¢å˜ä½“:`, allQueries)
    return allQueries
  } catch (error) {
    console.error('âŒ ç”ŸæˆæŸ¥è¯¢å˜ä½“å¤±è´¥:', error)
    // å¤±è´¥æ—¶è¿”å›åŸå§‹æŸ¥è¯¢
    return [originalQuery]
  }
}

/**
 * Multi-Query è¯­ä¹‰æœç´¢
 * 
 * ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢å˜ä½“è¿›è¡Œæœç´¢ï¼Œåˆå¹¶å»é‡ç»“æœ
 * 
 * @param query ç”¨æˆ·åŸå§‹æŸ¥è¯¢
 * @param limit æ¯ä¸ªæŸ¥è¯¢çš„ç»“æœæ•°é‡
 * @param threshold ç›¸ä¼¼åº¦é˜ˆå€¼
 * @param maxQueries æœ€å¤§æŸ¥è¯¢æ•°é‡ï¼ˆåŒ…å«åŸå§‹æŸ¥è¯¢ï¼‰
 * @returns åˆå¹¶åçš„æœç´¢ç»“æœ
 */
export async function multiQuerySearch(
  query: string,
  limit: number = 5,
  threshold: number = 0.3,
  maxQueries: number = 4
): Promise<SearchResult[]> {
  console.log(`ğŸš€ å¼€å§‹ Multi-Query æœç´¢: "${query.slice(0, 50)}..."`)

  // Step 1: ç”ŸæˆæŸ¥è¯¢å˜ä½“
  const queries = await generateQueryVariants(query, maxQueries)
  console.log(`ğŸ“ å…± ${queries.length} ä¸ªæŸ¥è¯¢`)

  // Step 2: å¹¶è¡Œç”Ÿæˆæ‰€æœ‰æŸ¥è¯¢çš„å‘é‡
  console.log(`ğŸ”¢ ç”ŸæˆæŸ¥è¯¢å‘é‡...`)
  const embeddings = await generateEmbeddingBatch(queries)
  
  // Step 3: å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢
  console.log(`ğŸ” å¹¶è¡Œæ‰§è¡Œæœç´¢...`)
  const supabase = await createClient()
  
  const searchPromises = embeddings.map((embedding, index) => 
    supabase.rpc('match_documents', {
      query_embedding: embedding,
      match_threshold: threshold,
      match_count: limit,
    }).then(({ data, error }) => {
      if (error) {
        console.warn(`âš ï¸ æŸ¥è¯¢ ${index + 1} æœç´¢å¤±è´¥:`, error.message)
        return []
      }
      console.log(`  æŸ¥è¯¢ ${index + 1} "${queries[index].slice(0, 20)}..." â†’ ${data?.length || 0} ä¸ªç»“æœ`)
      return data || []
    })
  )

  const allResults = await Promise.all(searchPromises)

  // Step 4: åˆå¹¶å’Œå»é‡ç»“æœ
  const mergedResults: SearchResult[] = []
  const seenChunks = new Set<string>()
  const chunkScores = new Map<string, number>()  // è®°å½•æ¯ä¸ª chunk çš„æœ€é«˜åˆ†

  for (const results of allResults) {
    for (const r of results) {
      const chunkId = r.chunk_id
      const similarity = r.similarity || 0

      // è®°å½•æœ€é«˜ç›¸ä¼¼åº¦
      if (!chunkScores.has(chunkId) || chunkScores.get(chunkId)! < similarity) {
        chunkScores.set(chunkId, similarity)
      }

      // å»é‡
      if (seenChunks.has(chunkId)) continue
      seenChunks.add(chunkId)

      mergedResults.push({
        chunkId: r.chunk_id,
        documentId: r.document_id,
        documentTitle: r.document_title,
        documentCategory: r.document_category,
        content: r.content,
        similarity: similarity,
      })
    }
  }

  // æ›´æ–°ç›¸ä¼¼åº¦ä¸ºæœ€é«˜åˆ†ï¼Œå¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
  for (const result of mergedResults) {
    result.similarity = chunkScores.get(result.chunkId) || result.similarity
  }
  mergedResults.sort((a, b) => b.similarity - a.similarity)

  console.log(`âœ… Multi-Query æœç´¢å®Œæˆ: ${mergedResults.length} ä¸ªå”¯ä¸€ç»“æœ`)
  return mergedResults
}

